{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define, random seed for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should load model and tokenizer. We choose Bert Base Turkish Cased as pretrained backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load dataset from conllu files by using conllu library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "def load_conllu_file(file_path):\n",
    "    \"\"\"Load a .conllu file and return parsed sentences.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = conllu.parse(f.read())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "sentences_train = load_conllu_file('ota_boun_ud-train.conllu')\n",
    "sentences_test = load_conllu_file('ota_boun-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should extract tokens and it's pos labels from datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_and_labels(sentences):\n",
    "    \"\"\"Extract tokens and POS tags from parsed sentences.\"\"\"\n",
    "    tokens_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        \n",
    "        for token in sentence:\n",
    "                \n",
    "            tokens.append(token['form'])\n",
    "            labels.append(token['upos'])\n",
    "        \n",
    "        if tokens:  # Only add non-empty sentences\n",
    "            tokens_list.append(tokens)\n",
    "            labels_list.append(labels)\n",
    "    \n",
    "    return tokens_list, labels_list\n",
    "\n",
    "training_tokens_list, training_labels_list = extract_tokens_and_labels(sentences_train)\n",
    "test_tokens_list, test_labels_list = extract_tokens_and_labels(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some initial attemps, we realize that some data augmentation is required , so  we decide to add IMST corpus to our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_imst= load_conllu_file('tr_boun-ud-train.conllu')\n",
    "training_tokens_list_imst, training_labels_list_imst = extract_tokens_and_labels(sentences_train_imst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training data from both sources\n",
    "training_tokens_list.extend(training_tokens_list_imst)\n",
    "training_labels_list.extend(training_labels_list_imst)\n",
    "\n",
    "print(f\"Combined training data: {len(training_tokens_list)} sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to determine unique pos labels and crate label2id, id2label dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for labels in training_labels_list:\n",
    "    for label in labels:\n",
    "        unique_labels.add(label)\n",
    "\n",
    "\n",
    "unique_labels = sorted(list(unique_labels))\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "print(f\"Found {len(unique_labels)} unique POS tags:\")\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max length.\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    " \n",
    "Now we'll create the dataset in the format required for BERT fine-tuning. This involves:\n",
    "1. Tokenizing the input text using the BERT tokenizer\n",
    "2. Aligning the POS labels with the tokenized input (handling subword tokenization)\n",
    "3. Padding sequences to a fixed length and creating attention masks\n",
    "4. Converting labels to numerical IDs and handling special tokens with -100 (ignored in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset in the required format\n",
    "def create_dataset(tokens_list, labels_list, tokenizer, label2id, max_length=512):\n",
    "    \"\"\"Create dataset with tokenized inputs and aligned labels.\"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list_aligned = []\n",
    "    \n",
    "    for tokens, labels in zip(tokens_list, labels_list):\n",
    "        # Tokenize the tokens\n",
    "        tokenized = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized['input_ids'].squeeze()\n",
    "        attention_mask = tokenized['attention_mask'].squeeze()\n",
    "        \n",
    "        # Align labels with tokenized input\n",
    "        word_ids = tokenized.word_ids()\n",
    "        aligned_labels = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                # Special tokens get -100 (ignored in loss calculation)\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # Map original label to id\n",
    "                aligned_labels.append(label2id[labels[word_id]])\n",
    "        \n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list_aligned.append(torch.tensor(aligned_labels))\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_list,\n",
    "        'attention_mask': attention_mask_list,\n",
    "        'labels': labels_list_aligned\n",
    "    }\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = create_dataset(training_tokens_list, training_labels_list, tokenizer, label2id)\n",
    "test_dataset = create_dataset(test_tokens_list, test_labels_list, tokenizer, label2id)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset['input_ids'])}\")\n",
    "print(f\"Test dataset size: {len(test_dataset['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the dataset to a Hugging Face Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "\n",
    "# Shuffle the training dataset\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "    \n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the compute_metrics function to compute the accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for POS tagging evaluation.\"\"\"\n",
    "    # We get the predictions from the model and labels from the dataset\n",
    "    predictions, labels = eval_pred\n",
    "    # By using argmax, we get the predicted label for each token\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    # We iterate over the predictions and labels and skip the special tokens\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for p, l in zip(prediction, label):\n",
    "            if l != -100:  # Skip special tokens\n",
    "                true_predictions.append(p)\n",
    "                true_labels.append(l)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, true_predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the embedding processor and sequence tagger.\n",
    "\n",
    "## 1) Embedding processor:\n",
    "- We use the BERT model to get the contextualized embeddings of the input tokens.\n",
    "- We apply dropout to the embeddings to regularize the model.\n",
    "- We apply token masking to the embeddings to regularize the model.\n",
    "- We follow \"BUILDING FOUNDATIONS FOR NATURAL LANGUAGE\n",
    "PROCESSING OF HISTORICAL TURKISH: RESOURCES AND\n",
    "MODELS\" paper for the choice of dropout rates. \n",
    "\n",
    "## 2) Sequence tagger:\n",
    "- We use a linear layer to project the output of the embedding processor to the number of POS tags.\n",
    "- We apply dropout to the output of the embedding processor to regularize the model.\n",
    "\n",
    "## 3) POS model:\n",
    "- We use the embedding processor and sequence tagger to create a POS model.\n",
    "- We use a (optional) BiLSTM layer to get the contextualized embeddings of the input tokens inspired by STEPS dependency parser architecture.\n",
    "- We apply dropout to the output of the BiLSTM layer to regularize the model.\n",
    "\n",
    "## 4) Loss function:\n",
    "- We use a loss function with label smoothing to regularize the model further.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "class EmbeddingProcessor(nn.Module):\n",
    "    \"\"\"BERT with specific dropout configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name, hidden_dropout=0.2, attention_dropout=0.2, \n",
    "                 output_dropout=0.5, token_mask_prob=0.15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.token_mask_prob = token_mask_prob\n",
    "        \n",
    "        # Override BERT's dropout settings\n",
    "        self.bert.config.hidden_dropout_prob = hidden_dropout\n",
    "        self.bert.config.attention_probs_dropout_prob = attention_dropout\n",
    "        \n",
    "        # Additional output dropout\n",
    "        self.output_dropout = nn.Dropout(output_dropout)\n",
    "        \n",
    "        # Apply new dropout settings to BERT layers\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            layer.attention.self.dropout = nn.Dropout(attention_dropout)\n",
    "            layer.attention.output.dropout = nn.Dropout(hidden_dropout)\n",
    "            layer.intermediate.dropout = nn.Dropout(hidden_dropout)\n",
    "            layer.output.dropout = nn.Dropout(hidden_dropout)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Apply token masking during training\n",
    "        if self.training and self.token_mask_prob > 0:\n",
    "            input_ids = self._apply_token_masking(input_ids, attention_mask)\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.output_dropout(outputs.last_hidden_state)\n",
    "        \n",
    "        return sequence_output\n",
    "    \n",
    "    def _apply_token_masking(self, input_ids, attention_mask):\n",
    "        \"\"\"Apply random token masking for regularization.\"\"\"\n",
    "        if attention_mask is None:\n",
    "            return input_ids\n",
    "            \n",
    "        mask_token_id = self.bert.config.vocab_size - 1\n",
    "        \n",
    "        # Create random mask (excluding special tokens)\n",
    "        rand = torch.rand_like(input_ids.float())\n",
    "        mask_prob = (rand < self.token_mask_prob) & (attention_mask == 1)\n",
    "        \n",
    "        # Exclude CLS and SEP tokens\n",
    "        mask_prob[:, 0] = False  # CLS\n",
    "        for i in range(mask_prob.size(0)):\n",
    "            seq_len = attention_mask[i].sum()\n",
    "            if seq_len > 1:\n",
    "                mask_prob[i, seq_len-1] = False  # SEP\n",
    "        \n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[mask_prob] = mask_token_id\n",
    "        \n",
    "        return masked_input_ids\n",
    "\n",
    "\n",
    "class SequenceTagger(nn.Module):\n",
    "    \"\"\"POS tagger with input dropout=0.2.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, num_labels, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(input_size, num_labels)\n",
    "        \n",
    "    def forward(self, sequence_output):\n",
    "        dropped_output = self.input_dropout(sequence_output)\n",
    "        logits = self.classifier(dropped_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class POSModel(nn.Module):\n",
    "    \"\"\" model for POS tagging with BiLSTM layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name, num_pos_labels, lstm_hidden_dim=256, use_bilstm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_labels = num_pos_labels\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim  \n",
    "        self.use_bilstm = use_bilstm\n",
    "        \n",
    "        # STEPS embedding processor with specified dropout rates\n",
    "        self.embedding_processor = EmbeddingProcessor(\n",
    "            bert_model_name=bert_model_name,\n",
    "            hidden_dropout=0.2,\n",
    "            attention_dropout=0.2,\n",
    "            output_dropout=0.5,\n",
    "            token_mask_prob=0.15\n",
    "        )\n",
    "        \n",
    "        bert_hidden_size = self.embedding_processor.bert.config.hidden_size\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=bert_hidden_size,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # POS sequence tagger with input dropout=0.2\n",
    "        # Input size should be 2*lstm_hidden_dim (bidirectional LSTM)\n",
    "        if use_bilstm:\n",
    "            self.pos_tagger = SequenceTagger(\n",
    "                input_size=2 * lstm_hidden_dim,  \n",
    "                num_labels=num_pos_labels,\n",
    "                dropout=0.2\n",
    "            )\n",
    "        else:\n",
    "            self.pos_tagger = SequenceTagger(\n",
    "                input_size=bert_hidden_size,\n",
    "                num_labels=num_pos_labels,\n",
    "                dropout=0.2\n",
    "            )\n",
    "        \n",
    "        # Loss function with label smoothing\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get contextualized embeddings from BERT\n",
    "        sequence_output = self.embedding_processor(input_ids, attention_mask)\n",
    "\n",
    "        if self.use_bilstm:\n",
    "            sequence_output, _ = self.bilstm(sequence_output)  # Get the output properly\n",
    "        \n",
    "        # POS tagging using BiLSTM output\n",
    "        logits = self.pos_tagger(sequence_output)  # Use lstm_output, not sequence_output\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the model and move it to the GPU. All experiments are done using 1x H100 GPU from Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By changing use_bilstm=True, we can use BiLSTM layer.Also lstm_hidden_dim can be changed.\n",
    "model = POSModel(\n",
    "    bert_model_name=\"dbmdz/bert-base-turkish-cased\",\n",
    "    num_pos_labels=len(label2id),\n",
    "    use_bilstm=False\n",
    ")\n",
    "device= \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the training arguments. There are 2 scenarios we can follow:\n",
    "1) Cold start training:\n",
    "- We train only the sequence tagger.\n",
    "- We use a learning rate of 3e-4.\n",
    "- We use a batch size of 32.\n",
    "- We use a max sequence length of 512.\n",
    "\n",
    "2) Fine-tuning:\n",
    "- We fine-tune the entire model on the train set.\n",
    "- We use a learning rate of 3e-5.\n",
    "- We use a batch size of 32.\n",
    "- We use a max sequence length of 512.\n",
    "- We use inverse square root learning rate scheduler with 400 warmup steps. (Inspired by paper: \"BUILDING FOUNDATIONS FOR NATURAL LANGUAGE\n",
    "PROCESSING OF HISTORICAL TURKISH: RESOURCES AND\n",
    "MODELS\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\"\"\"\n",
    "cold_start_training_args = TrainingArguments(\n",
    "    output_dir=f\"./results\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Custom trainer\n",
    "cold_start_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=cold_start_training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\"\"\" \n",
    "# Training arguments (might need adjustment for different heads)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results\",\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    lr_scheduler_type=\"inverse_sqrt\",\n",
    "    warmup_steps=400, \n",
    "    num_train_epochs=20,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Custom trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "### Cold start training for classification layers and fine-tuning the entire model. ###\n",
    "\n",
    "print(\"Cold start training for classification layers.\")\n",
    "for param in model.embedding_processor.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "cold_start_trainer.train()\n",
    "print(\"Step 2: Fine-tuning entire model...\")\n",
    "for param in model.embedding_processor.bert.parameters():\n",
    "    param.requires_grad = True\n",
    "trainer.train()\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"turkish_pos_model.pth\")\n",
    "print(\"Model saved as turkish_pos_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
